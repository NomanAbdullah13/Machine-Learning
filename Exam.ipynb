{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Solution 1:"
      ],
      "metadata": {
        "id": "ylkiIkPuAegy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def withdraw(balance, amount) :\n",
        "    try:\n",
        "        if amount > balance:\n",
        "            raise ValueError(\"Insufficient Funds.\")\n",
        "        return balance - amount\n",
        "\n",
        "    except ValueError as err:\n",
        "        print(err)\n",
        "        return balance"
      ],
      "metadata": {
        "id": "rmChRWlHCXsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution 2:"
      ],
      "metadata": {
        "id": "9Oa3gTtpEtpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate (num1, num2, operator) :\n",
        "    if operator == '+':\n",
        "        return num1 + num2\n",
        "    elif operator == '-':\n",
        "        return num1 - num2\n",
        "    elif operator == '*':\n",
        "        return num1 * num2\n",
        "    elif operator == '/' and num2 != 0 :\n",
        "        return num1 / num2\n",
        "    elif operator == '%' :\n",
        "        return num1 % num2\n",
        "\n",
        "    else:\n",
        "        return \"Invalid Operator.\""
      ],
      "metadata": {
        "id": "gShsdDW6GCXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution 3:"
      ],
      "metadata": {
        "id": "HevWnkuNH0n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "students = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"]\n",
        "Students_A_D_Name = []\n",
        "\n",
        "for SN in students:\n",
        "    if SN.startswith('A') or SN.startswith('D'):\n",
        "        Students_A_D_Name.append(SN)\n"
      ],
      "metadata": {
        "id": "1s9qVG0AH0KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Students_A_D_Name)"
      ],
      "metadata": {
        "id": "oZ4z9RUuI-nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution 4:"
      ],
      "metadata": {
        "id": "hBN0zmr5JLNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Person:\n",
        "    def introduce(self):\n",
        "        print(\"I am a person.\")\n",
        "\n",
        "class Student(Person):\n",
        "    def introduce(self):\n",
        "        print(\"I am a student.\")\n"
      ],
      "metadata": {
        "id": "T-DGwAEaJKru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person = Person()\n",
        "student = Student()\n",
        "\n",
        "person.introduce()\n",
        "student.introduce()"
      ],
      "metadata": {
        "id": "hRi_imEYKeDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution 7:"
      ],
      "metadata": {
        "id": "vYrIb-ZbPLRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the dataset**"
      ],
      "metadata": {
        "id": "pwtheUhxE3Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub pandas numpy nltk scikit-learn imblearn transformers tensorflow\n"
      ],
      "metadata": {
        "id": "A8GO6IYU3R90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "B8EGZQ7M1_2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#path\n",
        "df = pd.read_csv(f\"{path}/training.1600000.processed.noemoticon.csv\", encoding='latin-1', header=None)\n",
        "\n",
        "# Rename\n",
        "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
        "df = df[['sentiment', 'text']]\n",
        "\n",
        "# Convert label\n",
        "df['sentiment'] = df['sentiment'].replace({0: 0, 4: 1})"
      ],
      "metadata": {
        "id": "0uulDHs82s9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], dtype=tf.float32)\n",
        "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], dtype=tf.float32)\n",
        "    c = tf.matmul(a, b)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0Z0KFz7pNtBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "xqOUdN5CEjkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "#after processing\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "HkvGAkdJ3bK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# take 80/20 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train Size:\", len(X_train))\n",
        "print(\"Test Size:\", len(X_test))\n"
      ],
      "metadata": {
        "id": "p_m2sUUZ3ovr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF IDF vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(\"TF-IDF Train Shape:\", X_train_tfidf.shape)\n",
        "print(\"TF-IDF Test Shape:\", X_test_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "hMeCpoXF3tT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_tfidf_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "#After appling smote\n",
        "pd.Series(y_train_smote).value_counts()\n"
      ],
      "metadata": {
        "id": "IdOuvn0H3unT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# traditional ML model\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'SVM': SVC(kernel='linear', probability=True),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
        "}\n",
        "\n",
        "# used matrics\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_tfidf_smote, y_train_smote)\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"F1-score: {f1_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_test, model.predict_proba(X_test_tfidf)[:,1]):.4f}\")\n"
      ],
      "metadata": {
        "id": "7TBrpeE33xQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#hyperparameter\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "}\n",
        "\n",
        "# Initialize model and GridSearchCV\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='f1_macro', n_jobs=-1)\n",
        "grid_search.fit(X_train_tfidf_smote, y_train_smote)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters for Random Forest:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "hbF4Yfdt34q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Tokenization\n",
        "max_words = 5000\n",
        "max_len = 50\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)\n",
        "\n",
        "#lstm\n",
        "model = Sequential([\n",
        "    Embedding(max_words, 100, input_length=max_len),\n",
        "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train_seq, y_train, batch_size=32, epochs=5, validation_data=(X_test_seq, y_test))\n"
      ],
      "metadata": {
        "id": "ce0LLRlt36qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
        "mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
        "output = bert_model([input_ids, mask])[0]\n",
        "\n",
        "model_bert = tf.keras.Model(inputs=[input_ids, mask], outputs=output)\n",
        "model_bert.compile(optimizer=Adam(learning_rate=2e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\" DOne \")\n"
      ],
      "metadata": {
        "id": "nlGTg9G938Ox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}